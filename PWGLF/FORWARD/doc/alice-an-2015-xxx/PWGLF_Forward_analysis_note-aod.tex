%% === AOD output ====================================================
\section{Generating $\dndetadphi[i]$ event--by--event}
\label{sec:gen_aod}

When reading in the \ESD{}s and generating the $\dndetadphi$
event--by--event the following steps are taken (in order) for each
event $i$ and FMD ring $r$.
\begin{description}
\item[Event inspection] The global properties of the event is
  determined, including the trigger type and primary interaction
  point\footnote{`Vertex' and `primary interaction point' will be used
    interchangeably in the text, since there is no ambiguity with
    particle production vertex in this analysis.} $z$ coordinate (see
  \secref{sec:sub:event_inspection}).
%%
%% The footnote is kept to be explicit.  Better safe than sorry.
%% 
\item[\ESD{} fix-up] Addition of missing noise component, and filtering
  of `bad' strips (see \secref{sec:sub:fixer}).
\item[Sharing filter] The \ESD{} object is read in and corrected for
  sharing.  The result is a new \ESD{} object (see
  \secref{sec:sub:sharing_filter}).
\item[Density calculator] The sharing corrected\footnote{Optionally,
    the sharing filter can be turned off, in which case the density
    calculator will receive the output of the \ESD{} fixer.} \ESD{}
  object is then inspected and an inclusive\footnote{By `inclusive' we
    mean primary \emph{and} secondary particles.}, per--ring number of
  charged particles $\dndetadphi[incl,r,\subIPz{},i]$ is calculated.
  The output of this step are 5 $\etaphi$ histograms --- one per
  \FMD{} ring.  This calculation depends in general upon the
  interaction vertex position along the $z$ axis $\IPz{}$ (see
  \secref{sec:sub:density_calculator}).
\item[Corrections] The 5 $\dndetadphi[incl,r,\subIPz{},i]$ histograms
  are corrected for secondary production and acceptance.  The
  correction for the secondary particle production is highly dependent
  on the vertex $z$ coordinate.  The result is a per--ring, number of
  charged primary particles $\dndetadphi[r,\subIPz{},i]$ (see
  \secref{sec:sub:corrector}).  Note, that this step is omitted when
  the correction for secondaries is done using the empirical
  correction.   In that case, the output of the density calculator is
  passed straight to the next step. 
\item[Histogram collector] Finally, the 5 $\dndetadphi[r,\subIPz{},i]$
  (or $\dndetadphi[incl,r,\subIPz{},i]$ if the previous step was
  omitted) histograms are summed into a single
  $\dndetadphi[\subIPz{},i]$ histogram, taking care of the overlaps
  between the detector rings.  In principle, this histogram is
  independent of the vertex, except that the pseudo--rapidity range,
  and possible holes in that range, depends on $\IPz{}$ (see
  \secref{sec:sub:hist_collector}).
\end{description}

Each of these steps will be detailed in the following. 

\subsection{Event inspection}
\label{sec:sub:event_inspection}

\paragraph{Trigger} 
The first thing to do, is to inspect the event for triggers.  A number
of trigger bits, like \INEL{} (Minimum Bias for Pb+Pb), \INELONE{},
\NSD{}, and so on are propagated to the \AOD{} output.

\paragraph{Pile--up} 
The event is checked for pile-up using up three different
methods\footnote{The standard methods \texttt{IsPileupFromSPD} from
  \texttt{AliESDEvent}, and methods from \texttt{AliAnalysisUtils} is
  used.} 
\begin{itemize}
\item \SPD{} pile--up flag from multiple interaction points
\item Track pile--up flag from multiple interaction points
\item Out--of--bunch pile--up flag from the beam parameters 
\end{itemize}
The pile--up status is propagated to the \AOD{} output.  

\paragraph{\SPD{} outlier} 
The number of clusters in the \SPD{} is compared to the number of
tracklets from the \SPD{}, and events with too many clusters compared
to the number of tracklets is flagged in the \AOD{} as an outlier
event\footnote{The equivalent of the method
  \texttt{IsSPDClusterVsTrackletBG} of \texttt{AliAnalysisUtils} is
  used.  See also
  \href{https://groups.cern.ch/group/alice-analysis-operations/Lists/Archive/Flat.aspx?RootFolder=\%2Fgroup\%2Falice-analysis-operations\%2FLists\%2FArchive\%2FSPD\%20cluster-vs-tracklet\%20cut\%20for\%20background\%20rejection&FolderCTID=0x01200200726CA2ECC2B5D14F9300A07F2C08D3A2}{this post}.}
%% 
%% The SPD pile-up check was implicit in earlier version of
%% AliPhysicsSelction up to LHC10x, pass2.  Since then it has been
%% removed and must explicitly be done by the user.  See also 
%% https://groups.cern.ch/group/alice-analysis-operations/Lists/Archive/Flat.aspx?RootFolder=%2Fgroup%2Falice-analysis-operations%2FLists%2FArchive%2FSPD%20cluster-vs-tracklet%20cut%20for%20background%20rejection&FolderCTID=0x01200200726CA2ECC2B5D14F9300A07F2C08D3A2
%% 

\paragraph{Interaction point $z$ coordinate ($\IPz$)} 
Depending on the settings, the interaction point coordinates is
queried from the input \ESD{} and propagated to the \AOD{}.  In
general, and \SPD{} primary vertex is required, but this can be
changed if needed. 

Just after the sharing filter (see \secref{sec:sub:sharing_filter})
but before any further processing, the interaction point information
is queried.  If there is no information, or if the interaction point
$z$ coordinate ($\IPz$) is outside the pre--defined range, then no
further processing of that event takes place.

\subsubsection{Satellite Vertices}
\label{sec:sub:sub:dispvtx}

The analysis can be set up to run on the `displaced vertices' that
occur during \LHC{} 2010 \PbPbCol{} running. Details on the displaced
vertices, and their selection can be found in the \VZERO{} analysis note
\cite{maxime}. In \secref{sec:sub:empirical} we detail how these
events is used to form the \emph{empirical} correction for secondary
particle production.

\subsection{\ESD{} fix-up}
\label{sec:sub:fixer} 

For the 2010--'13 data, the \FMD{} strip signals in the \ESD{} are not
aligned at the real zero, due to a bug in the reconstruction code.  In
the reconstruction pass only a $1\times\sigma$ ($\sigma$ is the noise
in a given strip) was added, but the electronics had suppressed
$4\times\sigma$.  To align at the proper zero, we therefor add
$3\times\sigma$ scaled by the gain factor $\DeltaMip$.  This is
entirely understood and the fix is implemented to automatically
correct the zero signal if needed. 

For some of the earlier runs, the off--line shuttle\footnote{The
  software that processes only conditions and writes these to the
  Off--line Conditions DataBase (OCDB).}\ did not properly flag certain
strips as `bad'.  We therefore flag additional problematic strips in
this pass as `bad'.  `Bad' in this context means high noise or low
gain, or a noise--to--gain factor larger than some cut-off (typically
0.4). 

The fixes applied in this step are entirely mechanical and corresponds
to well understood issues.  The result of this step is a modified
\ESD{} object which is passed on in to the next step in the chain.


\subsection{Sharing filter}
\label{sec:sub:sharing_filter}

A particle originating from the vertex can, because of its incident
angle on the \FMD{} sensors, traverse more than one strip (see
\figref{fig:share_fraction}).  This means that the energy loss of the
particle is distributed over 1 or more strips.  To reconstruct the
energy loss of a single particle, one may possibly need to merge the
signals of adjacent strips.

\begin{figure}[htbp]
  \centering
  \figinput[3cm]{share_fraction}
  \caption{A particle traversing 2 strips and depositing energy in
    each strip. }
  \label{fig:share_fraction}
\end{figure}

\iffalse
%% 
%% Since we no longer distinguish between high or low flux events,
%% this paragraph is largely irrelevant and therefor removed
%%
The effect is most pronounced in low--flux\footnote{Events with a low
  hit density.} events, like proton--proton collisions or peripheral
Pb--Pb collisions, while in high--flux events the hit density is so
high that most likely each and every strip will be hit and the effect
cancels out on average.
\fi

Since the particles travel more or less in straight lines toward the
\FMD{} sensors, the sharing effect is predominantly in the $r$ or
\emph{strip} direction.  Only neighbouring strips in a given sector are
therefore investigated for this effect.  

Algorithm~\ref{algo:sharing}\footnote{The algorithm is shown here
  almost as it is coded.  As it stands here, it could easily be
  simplified.  However, in the code (\texttt{AliFMDSharingFilter})
  there are side--effects and settings which will influence the
  flow. In particular, Algorithm~\ref{algo:sharing} shows the case
  where at most 2 strips can be merged (default setting), but it is
  possible for the code to merge over 2 or 3 strips.} is applied to
the signals in a given sector. Here the function
\FuncSty{SignalInStrip}($t$) returns the properly path--length
corrected signal in strip $t$.  Two cuts are used in this algorithm:
\begin{description}
\item[\FuncSty{GetLowCut}] Signals below this cut is considered
  pedestal remnants and are never merged. 
\item[\FuncSty{GetHighCut}] Signals above this cut are considered
  isolated hits, but can be merged with a neighbouring signal below
  this cut. 
\end{description}
The exact value returned by the two functions  \FuncSty{GetLowCut} and
\FuncSty{GetHighCut} and how it those values are obtained can be
varied in the analysis (see also \secref{sec:cuts}).  


\begin{algorithm}[htpb]
  \belowpdfbookmark{Algorithm 1}{algo:sharing}
  \SetKwData{used}{u}
  \SetKwData{Output}{o${}_t$}
  \SetKwData{Input}{i}
  \SetKwData{Nstr}{N}
  \SetKwData{Signal}{c}
  \SetKwData{Sum}{s}
  \SetKwData{Eta}{$\eta$}
  \SetKwData{Low}{l}
  \SetKwData{High}{h}
  \SetKwData{Next}{n} 
  \SetKwData{Tmp}{t} 
  \SetKw{continue}{continue}
  \SetCommentSty{textit}
  \SetKwComment{Cm}{}{}
  \SetKwData{assign}{\ifmmode\leftarrow\else$\leftarrow$\fi}
  %% \SetKwData{nextNextE}{next-to-next strip signal} 
  \SetKwFunction{SignalInStrip}{SignalInStrip}
  \SetKwFunction{GetLowCut}{GetLowCut}
  \SetKwFunction{GetHighCut}{GetHighCut}
  \used \assign false\Cm*{Previous strip used?}
  \Sum \assign -1\Cm*{Current sum}
  \For(\Cm*[f]{Loop over \Nstr strips}){$t$ \assign $1$ \KwTo \Nstr}{ 
    \Output\assign 0\Cm*{Default result}
    \Signal \assign \SignalInStrip($t$)\Cm*{Current strip signal}
    \uIf{$t \ne$ \Nstr}{% 
      \Next \assign \SignalInStrip($t+1$)\Cm*{Next strip signal}
    }
    \Else{% 
      \Next \assign 0\;
    }
    \lIf(\Cm*[f]{If the next is `bad'}){\Next is not valid}{
      \Next \assign 0}
    \Eta\assign $\eta$ of $t$\Cm*{Current strip $\eta$}
    %% 
    \lIf(\Cm*[f]{If `bad}){\Signal is not valid}{%
      \Output \assign invalid} 
    \If(\Cm*[f]{Null or
      invalid signal}){\Signal is not valid $\vee$ \Signal $=$ 0}{%
      
      \lIf(\Cm*[f]{If current sum is non-zero, flush it}){\Sum $>$ 0
        $\wedge$ $t\ne1$}{%
        \Output \assign \Sum        
      }
      \Sum \assign -1\Cm*{Clear flag and sum}
      \used \assign false\;
      \continue\Cm*{To next strip}
    }
    \Low \assign\GetLowCut(\Eta)\Cm*{Get low cut}
    \High \assign\GetHighCut(\Eta)\Cm*{Get high cut}
    \Tmp \assign 0\Cm*{Figure out what to take}
    \uIf(\Cm*[f]{If we already have some}){\Sum $>$ 0}{% 
      \Tmp \assign \Sum\Cm*{Then take that}
      \Sum \assign -1\Cm*{Clear flag and sum}
      \used \assign false\;
    }
    \Else(\Cm*[f]{No current sum}){% 
      \If(\Cm*[f]{Already used}){\used}{%
        \used \assign false\;
        \continue\Cm*{Move on to the next}
      }
      \lIf(\Cm*[f]{Not noise}){\Signal $>$ \Low}{\Tmp \assign \Signal}
      \If(\Cm*[f]{This and next not noise, and one is small}){%
        \Signal $>$ \Low $\wedge$ \Next $>$ \Low $\wedge$ 
        (\Signal $<$ \High $\vee$ \Next $<$ \High)}{% 
        \uIf(\Cm*[f]{Which is larger?}){\Signal $>$ \Next}{% 
          \Tmp \assign \Signal $+$ \Next\Cm*{This larger, assign here}
          \used \assign true\Cm*{Do not take this on next pass}
        }
        \Else{ 
          \Tmp \assign 0\Cm*{Next larger, assign next iteration}
          \Sum \assign \Signal $+$ \Next\;
        }
      }
    }
    \Output \assign \Tmp\Cm*{Assign output for this strip}
  }
  \caption{Sharing correction}
  \label{algo:sharing}
\end{algorithm}


In short, each strip \DataSty{t} of a radial sector is investigated in
turn.  
\begin{itemize}
\item If a strip signal \DataSty{c} is invalid, the output for that is
  set to invalid, and the current sum flushed (lines 12 to 17).
\item If a strip signal is below \DataSty{l}, then output of that
  strip is set to null and the current sum flushed (lines 13 to 17).
\item If we have a non--null sum \DataSty{s}, then that value is used
  as this strip signal (lines 21 to 24).  
\item If a strip is flagged as used already, the output for that strip
  is set to null (lines 26 to 28).
\item If the signal \DataSty{c} is larger than the cut defined by
  \DataSty{l} it is considered a candidate for merging (line 29).
  Depending on the signal in the next strip (\DataSty{n}) a number of
  possible things happen.
  \begin{itemize}
  \item \DataSty{c} $<$ \DataSty{h} or \DataSty{n} $<$ \DataSty{h}
    (line 30), then
    \begin{itemize}
    \item \DataSty{c} $>$ \DataSty{n} then the two signals are added
      and output that as this strip \DataSty{t} signal.  The current
      sum \DataSty{s} is reset and the next strip is flagged as used
      (lines 31 to 33).
    \item \DataSty{c} $\le$ \DataSty{n} then the strip signals
      \DataSty{c} and \DataSty{n} is added to the current sum
      \DataSty{s}, and the value of this strip is set to null, and the
      next strip flagged as used (lines 34 to 36).
    \end{itemize}
  \item Otherwise, the current strip signal \DataSty{c} is output
    as--is.
  \end{itemize}
\end{itemize}
The result of this pass is a new \ESD{} object where signals that are
spread of two strips have been merged into one (see also
\figref{fig:sharing:effect}).

\begin{figure}[htbp]
  \centering
  \figinput[.9\linewidth]{sharing}
  \caption{The effect of the sharing filter.  The light distributions
    are from the reconstruction, while the darker distributions are
    after the algorithm has been applied.  The yellow band indicates
    the lower excluded region, while the cyan band illustrates the
    spread of the high cut. }
  \label{fig:sharing:effect}
  %% There's no point in adding a ratio plot here, even if the
  %% difference between the two distributions is hard to see over some
  %% value.  The point of this figure is really in the low part. 
\end{figure}

%% \clearpage

\subsection{Density calculator}
\label{sec:sub:density_calculator}

The density calculator loops over all the strip signals in the sharing
corrected\footnote{The sharing correction can be disabled, in which
  case the density calculator uses the input \ESD{} signals.} \ESD{}
and calculates the inclusive (primary + secondary) charged particle
density in pre--defined $\etaphi$ bins.  The output is stored in 5
$\etaphi$ binned histograms --- one for each of the sub-detectors
\FMD{1i}, 2i, 2o, 3o, and 3i.  The exact binning of the histograms
depends on settings, but default values are listed in
\tabref{tab:etaphi:binning}.

\begin{table}[h!tbp]
  \centering
  \caption{Default $\etaphi$ binning}
  \begin{tabular}[t]{|c|ccc|ccc|}
    \hline
    \headColor%
    \textbf{Sub-} & \multicolumn{3}{c|}{$\mathbf{\eta}$} & 
    \multicolumn{3}{c|}{$\mathbf{\varphi}$}\\
    \headColor%
    \textbf{detector} & \# bins & Min. & Max. & \# bins & Min. & Max. \\
    \hline
    \FMD{1i},2i,3i & 200 & -4  & 6 & 20 & 0 & $2\pi$ \\
    \altRowColor%
    \FMD{2o},3o    & 200 & -4  & 6 & 40 & 0 & $2\pi$ \\
    \hline
  \end{tabular}
  \label{tab:etaphi:binning}
\end{table}

To evaluate the charge particle multiplicity for a given strip $t$, we
evaluate the weighted energy loss fits (see \secref{sec:fits}) at the
reconstructed (possibly merged) energy loss $\Delta_t$ of the strip 
\begin{align}
  \label{eq:nt}
  n_t &= G_{N_{\text{max}}}(\Delta_t;\Delta_p,\xi,\sigma,\mathbf{a}) =
  \frac{\sum_i^{N_{max}} i\,a_i\,f_i(\Delta_t;\Delta_p,\xi,\sigma)}{
    \sum_i^{N_{max}}\,a_i\,f_i(\Delta_t;\Delta_p,\xi,\sigma)}\quad,
\end{align}
That is, the $\Nch$ weighted average of the fit to the energy loss
distributions at the given energy loss $\Delta_t$. 

The sensors of the \FMD{} are not perfect arc--segments
\cite{cholm:2009} --- the two top corners are cut off to allow the
largest possible sensor on a 6'' Si-wafer.  This means, however, that
the strips in these outer regions do not fully cover $2\pi$ in
azimuth, and there is therefore a need to correct for this limited
acceptance.

The correction is only applicable where the strip length does not
cover the full sector.  This is the case for the outer strips in both
the inner and outer type rings.  The acceptance correction is then
simply
\begin{align}
  \label{eq:acc_corr}
  \Corners{} &=
     \left\{
       \begin{array}{cl}
         1 & \text{for non-cut strips}\\
         \Delta\varphi/l_t & \\
       \end{array}\right.
     \quad
\end{align}
where $l_t$ is the strip length in radians at constant $r$, and
$\Delta\varphi$ is $2\pi$ divided by the number of sectors in the ring
(20 for inner type rings, and 40 for outer type rings). Note, that
this correction is a hardware--related correction, and does not depend
on the properties of the collision (e.g., primary vertex location).

We then define the $\varphi$--acceptance corrected signal in a strip
$t$ as 
\begin{align}
  n'_t &= \Corners{} n_t
\end{align}

To fill the 5 per--ring, per--event, $\etaphi$, vertex-dependent
output histograms $\dndetadphi[incl,r,\subIPz,i]$ we can proceed in
one of two ways detailed in the following.

\subsubsection{Inclusive number of charged particles: Energy Fits} 
\label{sec:sub:sub:eloss_fits}

For this method, we simply sum up $n'_t$ for all strips that fall
within an $\etaphi$ bin.  
\begin{align}
  \label{eq:density1}
  \dndetadphi[incl,r,\subIPz,i] &= \sum_t^{t\in\etaphi} n'_t
\end{align}
where $t$ runs over the strips in the $\etaphi$ bin. 

This method has the advantage that it takes into account the energy
loss $\Delta_t$ of each strip.  The down--side of the approach is its
sensitivity to the event sample to which $f_N(xt;\Delta_p,\xi,\sigma)$
where fitted.  For low--multiplicity environments such as \ppCol{} the
sensitivity is relatively small, while this method implies a bias
toward the minimum bias sample for larger multiplicities.

\subsubsection{Inclusive number of charged particles: Poisson Approach} 
\label{sec:sub:sub:poisson}

% \newcommand\muR{\ensuremath\mu_R}
\newcommand\muR{\ensuremath\mu} Another approach to the calculation of
the number of charged particles is using Poisson statistics. This is
the default choice as it is less sensitive to the stability of the
fits and the underlying data than required for the energy fits method.

Firstly we define a strip $t$ as being `hit' if 
\begin{align*}
  t_{\text{hit}} &= \left\{ 
    \begin{array}{cl}
      1 & \Delta_t > c(\eta_t) \wedge\ n'_t > 0.9\\
      0 & 0 \leq \Delta_t \leq c(\eta_t)\vee\  0 \leq \vee n'_t \leq 0.9\\
      \text{undefined} & \text{otherwise}\\ 
    \end{array}\right.
\end{align*}
where $c(\eta_t)$ is some cut that defines a lower threshold for when
a signal is considered a `hit' (see also \secref{sec:cuts}).  For a
given set of strips $R$ we then define 
\begin{align*}
  \Nempty{} &= \sum_t^{t\in R\wedge\ t_{hit}=0}\\
  \Ntotal{} &= \sum_t^{t\in R\wedge\ t_{hit}\neq\text{undefined}}\quad,\\
\end{align*}
that is, the number of strips in $R$ with a valid signal below the
threshold and the number of strips with a valid (possibly 0) signal. 

Now, assume in the region $R$ the $\mult$ is distributed according to
a Poisson distribution with mean $\muR$.  This implies the
probability of $\mult=n$ is:
\begin{equation}
  P(n) = \frac{\muR^n e^{-\muR}}{n!} \label{eq:PoissonDef}
\end{equation}
In particular we have 
\begin{align*}
  \frac{\Nempty{}}{\Ntotal{}} &=   P(0) = e^{-\muR} \\
\intertext{and we find}
   \muR &= -\log\left(\frac{\Nempty{}}{\Ntotal{}}\right) = -\log(f)\quad,
\end{align*}
with $f=\Nempty{}/\Ntotal{}$.  That means we can calculate the mean
number of particles per $\etaphi$ bin of a larger $\etaphi$ region by
simply count the number of strips with no signal\footnote{Or rather, a
  signal below threshold.}.

The expected number of particles in a strip \emph{with a signal above
  threshold} is then given by
\begin{align}
  N_{\text{expected},R} &= \frac{\sum_{n>0}nP(n)}{\sum_{n>0}P(n)} 
  =  \frac1{1-P(0)}\sum_{n>0}n\frac{\muR^n}{n!}e^{-\muR} \nonumber\\
  \intertext{pulling out $\muR e^{-\muR}$}\nonumber
  &= \frac{e^{-\muR}}{1-e^{-\muR}}\muR\sum_{n>0}\frac{\muR^{n-1}}{(n-1)!} 
  =
  \frac{e^{-\muR}}{1-e^{-\muR}}\muR\sum_{m=0}\frac{\muR^m}{m!}\\
  \intertext{Identifying the sum $\sum_m\muR^m/m! = e^{\muR}$, we get}\nonumber
  &= \frac{e^{-\muR}}{1-e^{-\muR}}\muR e^{\muR}  = 
  \frac{\muR}{1-e^{-\muR}}\nonumber\\
  \intertext{Inserting $\muR=-\log(f)$ gives}\nonumber
  &=
  \frac{-\log(f)}{1-e^{\log(f})}
   =  \frac{-\log(f)}{1-f}
\end{align}
and the final number of particles in each of the $\etaphi$ bin of the
defined region becomes 
\begin{align}
  \label{eq:density2}
  \dndetadphi[incl,r,\subIPz,i] &= \Nhit{}\,N_{\text{expected},R} 
%% The below only applies for the case where the region is exactly the
%% size of the (eta,phi)
%% = 
%%  (\Ntotal-\Nempty)\frac{-\log\left(\frac{\Nempty{}}{\Ntotal{}}\right)}{1-\frac{\Nempty{}}{\Ntotal{}}} 
%%   = \Ntotal{}\log\left(\frac{\Nempty{}}{\Ntotal{}}\right)
\end{align}
where $\Nhit{}$ is the number of strips in the given $\etaphi$ bin
\emph{with} a signal above the defined threshold.  The uncertainty is
defined by the the Poisson calculation as set to $\sqrt{\Nch}$. 

The size of the regions used can be varied in the analysis.  The
default is $\unit[32]{strips}\times\unit[4]{sectors}$ (see also
\tabref{tab:fmd:overview}).  The exact definition of the threshold can
likewise be varied (see also \secref{sec:cuts}). 

\begin{figure}[htbp]
  \centering
  \figinput[.9\linewidth]{corr_method}
  \caption{Correlation of the two methods to estimate
    $\dndetadphi[incl,r,\subIPz,i]$.  The spread for all sub--detectors is
    less than 1\%.}
  \label{fig:density:corr}
\end{figure}


The Poisson method and the energy fits method have been compared in
\cite{hhd:2009} where it is found that the two methods are in good
agreement. The residual difference between the methods contributes to
the systematic error (see also \figref{fig:density:corr}).

\subsubsection{Azimuthal ($\varphi$) Acceptance} 

For a given $\eta$ bin, the calculation
$\dndetadphi[incl,r,\subIPz,i]$ also counts the number of strips with
$t_{hit}\neq\text{undefined}$, and the total number of strips, and then
forms the ratio
\begin{align}
  \label{eq:overflow}
  \phiAcc &= \frac{\sum_t^{t\in\eta \wedge
      t\neq\text{undefined}}}{\sum_t^{t\in\eta}} \quad,
\end{align}
which is the per--event, per $\eta$ bin azimuthal acceptance.  This
number is stored in the corresponding overflow bin of the 5 output
$\etaphi$ histograms, and can later be used to correct for the
$\varphi$ acceptance.

\subsection{Corrections}
\label{sec:sub:corrector}

\begin{center}
  \begin{tabular}{|p{.9\linewidth}|}
    \arrayrulecolor{alicepurple}
    \hline
    \cellcolor{alicered!20}\\
    \cellcolor{alicered!20}
    The corrections described in this section are by default turned off.
    We outline them here mainly for historical and reference
    purposes. That means that 
    \begin{equation}
      \dndetadphi[r,\subIPz,i] = \dndetadphi[incl,r,\subIPz,i]
    \end{equation}\\
    \hline
  \end{tabular}
\end{center}

The corrections code receives the five vertex dependent,
per--ring histograms of the inclusive charged particle density
$\dndetadphi[incl,r,\subIPz,i]$ from the density calculator and applies
up to two corrections.   

\subsubsection{Secondary correction}
\label{sec:sub:sub:secmap}
%%
%%                hHits_FMD<d><r>_vtx<v> 
%% hCorrection = -----------------------
%%                hPrimary_FMD_<r>_vtx<v>
%%
%% where 
%% - hPrimary_FMD_<r>_vtx<vtx> is 2D of eta,phi for all primary ch
%%   particles
%% - hHits_FMD<d><r>_vtx<v>  is 2D of eta,phi for all track-refs that
%%   hit the FMD - The 2D version of hMCHits_nocuts_FMD<d><r>_vtx<v>
%%   used below. 
This is a 2 dimensional histogram generated from simulations, as the
ratio of the total number of particles to the number of primary
particles that fall within an $\etaphi$ bin for a given vertex bin

\begin{align}
  \label{eq:secondary}
  \SecMap{} &=
  \frac{\sum_i^{\NV[,\subIPz]}\mult[,\text{\FMD{}},i]\etaphi}{
    \sum_i^{\NV[,\subIPz]}\mult[,\text{primary},i]\etaphi}\quad,
\end{align}
where 


\begin{tabular}[T]{p{.18\linewidth}p{.78\linewidth}}
  $\NV[,\subIPz]$ & is the number of events with a valid trigger
                    and a vertex in bin $\IPz$.\\ 
  $\dndetadphi[\FMD{},i]$ & is the total number of charged
                            particles that hit the \FMD{} in event $i$
                            in the specified $\etaphi$ bin.  This
                            number is counted from the simulation, and
                            not  after the
                            reconstruction\footnotemark.\\  
  $\dndetadphi[\text{primary},i]$ & is number of primary charged
                                    particles in event $i$ within the
                                    specified $\etaphi$ bin.  This is
                                    counted \emph{at the time of the
                                    collision} as defined in the
                                    simulation code.  That is, it is
                                    the number of primaries within the
                                    $\etaphi$ bin at the collision
                                    point --- not at the \FMD{}. 
\end{tabular}
\footnotetext{Technically, we are counting `track references' within
  the $\etaphi$ bin for a given sub-detector.}
\iffalse
\begin{description}
\item[] $\NV[,\subIPz]$ is the number of events with a valid trigger
  and a vertex in bin $\IPz$.
\item[] $\dndetadphi[\FMD{},i]$ is the total number of charged
  particles that hit the \FMD{} in event $i$ in the specified
  $\etaphi$ bin.  This number is counted from the simulation, and not
  after the reconstruction\footnote{Technically, we are counting
    `track references' within the $\etaphi$ bin for a given
    sub-detector.}.
\item[] $\dndetadphi[\text{primary},i]$ is number of primary charged
  particles in event $i$ within the specified $\etaphi$ bin.  This is
  counted \emph{at the time of the collision} as defined in the
  simulation code.  That is, it is the number of primaries within the
  $\etaphi$ bin at the collision point --- not at the \FMD{}.
\end{description}
\fi

$\SecMap$ varies from $\approx 1.5$ for the most forward bins to
$\approx 3$ for the more central bins. \figref{fig:secondaries:origin}
shows the $\dndeta$ of secondaries from various sources assessed with
simulations to give an idea of the magnitude of the effects of
secondaries.

\begin{figure}[htbp]
  \centering
  \figinput[.9\textwidth]{secondary_origin}
  \caption{$\dndeta$ for secondaries and primaries in the \FMD{} and
    \SPD{}, with origin of secondaries shown.}
  \label{fig:secondaries:origin}
\end{figure} 

\begin{figure}[htbp]
  \centering
  \figinput[.8\textwidth]{secmap_pp_aa_one}
  \caption{Top shows projection of $\SecMap$ onto the $\eta$ axis for
    one interaction point bin ($\unit[0]{cm}<\IPz{}<\unit[2]{cm}$).
    The different colours correspond the 5 \FMD{} rings.  The
    different symbols correspond to different collisions systems
    simulated in \texttt{LHC14i2}.  The bottom shows the ratio
    $\SecMap$ for each collision system to the average $\SecMap$ over
    all collision systems. All systems fall with--in 5\%, except
    \PbpCol{} and \PbPbCol{} in \FMD{1} which deviates with up to
    10\%. }
  \label{fig:secmap:all}
\end{figure} 
 
The systematics of the secondary correction was evaluated through a
comprehensive simulation production
(\href{https://alimonitor.cern.ch/job_details.jsp?jt_field1=LHC14i2}{%
  \texttt{LHC14i2}}) where all collision systems where simulated with
the same underlying geometric description. An example of that study is
shown in \figref{fig:secmap:all}.  It is seen, that the secondary map
is independent of the particle multiplicity, as one would expect from
how the track propagation code (\GEANT{3}) works: Each track is
propagated independent of other particles --- \textit{i.e.}, one will
get the same value of $\SecMap$ from $N$ events with $n$ particles as
one would from a single event with $N\times n$ particles, or
$N\times n$ events with a single particle.

Overall, the secondary maps for different collision systems and
energies agree within at least 5\% --- except for \PbPbCol{} and
\PbpCol{} which deviates more in the region of \FMD{1}.  This effect
was studied in detail using the above mentioned simulation
production.  The effect was found to be a consequence of the different
hadron chemistry encoded in each of the underlying event
generators\footnote{Pythia6 was used for all \ppCol{} energies, DpmJet
  for \pPbCol{}/\PbpCol{}, and Hijing for \PbPbCol{}.} and the
resulting primary particle distribution. 

To see this, a study was performed, selecting the \ppCol{} at
\usNN{pp}{0900} as the baseline.  In the study each particle (primary
and secondary) entering \eqref{eq:secondary} was weighted according to
their ultimate primary particle $\eta$ and $\pT$.  Particle weights
where assigned to the following set of particle (and anti--particle)
types\footnote{The weights for $e$, $\mu$, were the same.  Most of the
  primary photons come from $pi^{0}$ decays so it is possible to
  weight neutral pions produced by the event generator not photons.}:
%%
$\gamma$, 
$e^\pm$,
$\mu^\pm$,
$\pi^\pm$,
$K^0_{s}$, 
$K^\pm$,
$p$,
$\bar{p}$,
$n$,
$\Lambda$,
$\Sigma^\pm$,
$\Omega^\pm$,
$\Xi^\pm$, and
$\Xi^{0}$.
%% 
The weight for a given particle type $i$ for system $j$ is given by 

\begin{align}
  \label{eq:weights}
  W_{i}= \left(
  \frac1{\Nch}\diffFrac{^{2}N_{i}}{\pT\mathrm{d}y} \right)_{%
  \text{\ppCol{}},\GeV[900]{}} \Big/ 
  \left(\frac1{\Nch}\diffFrac{^{2}N_{i}}{\pT{}\mathrm{d}y}\right)_{j}\quad.
\end{align}

\begin{figure}[h!tbp]
  \centering
  \figinput[.8\textwidth]{310weights}
  \caption{The $K^{0}_{s}$ weights for \PbPbCol{} calculated based on
    \eqref{eq:weights}.  Top--left and bottom--right panels show the
    averaged weights for some pseudorapidity and transverse momentum
    slices as indicated by the dashed lines on the bottom left.}
  \label{fig:weights}
\end{figure} 

\begin{figure}[h!tbp]
 \centering
  \figinput[.8\textwidth]{secmap_pp_aa_one_weighted}
  \caption{Same as \figref{fig:secmap:all}, but with the particle weights
    of \eqref{eq:weights} applied. Top: projection of the weighted
    $\SecMap$ onto the $\eta$ axis for one interaction point bin
    ($\unit[0]{cm}<\IPz{}<\unit[2]{cm}$).  The different colours
    correspond the 5 \FMD{} rings.  The different symbols correspond
    to different collisions systems simulated in \texttt{LHC14i2}.
    The bottom shows the ratio those $\SecMap$ for each collision
    system to the average $\SecMap$ over all collision systems. All
    systems fall within 2\% after applying weights to account for the
    different hadron chemistry of the event generators.}
  \label{fig:secmapweighted:all}
\end{figure} 

The binning of weights depended on available statistic for a given
particle species.  \figref {fig:weights} shows an example of the
weights for $K^{0}_{s}$ for
\PbPbCol{}. \figref{fig:secmapweighted:all} shows the comparison of
the secondary maps from the weighting procedure for all the system. It
is clear that all discrepancies observed in
\figref{fig:secmap:all} are compressed by the weighting procedure.
The effect of this sensitivity to the hadron--chemistry is accounted
for in the systematic uncertainties (see
\secref{sec:sub:dndeta:syserr}), with an additional component for the
region of \FMD{1}. 

%% 
%% We presume that FMD1 is most effected because of 
%%
%%  - Large bin-flow of secondaries - in particular in FMD1
%%  - Larger slope on dN/deta in that region which tends to accentuate
%%    the effect of the hadron--chemistry. 
%%
\FIXME{Conclusion on this}
%% 
%% Marek is looking at the real-data weights.  We should compare these
%% to our MC weights to get an idea of how realistic those MC weights
%% really are. 
%% 

%For pp, different event
%generators were used and found to give compatible results within
%3--5\%.   
For \ppCol{}, at least some millions of events must be accumulated to
reach satisfactory statistics.  For \PbPbCol{} where the general hit
density is larger, reasonable statistics can be achieved with less
simulated data. \pPbCol{}/\PbpCol{} sits somewhere in between. 

\subsubsection{Acceptance due to dead channels}

Some of the strips in the \FMD{} have been marked up as \emph{dead},
meaning that they are not used in the reconstruction or analysis.
This leaves holes in the acceptance of each defined $\etaphi$ which
need to be corrected for.

The map of strips marked as `bad' is stored in the off-line conditions
database (\OCDB{}), and based on this information and the geometric
description we can built up per--vertex bin $\IPz$ acceptance maps
\begin{align}
  \label{eq:dead_channels} 
  \DeadCh{} &= 
  \frac{\sum_t^{t\in\etaphi}\left\{\begin{array}{cl}
        1 & \text{if `bad'}\\
        0 & \text{otherwise}
      \end{array}\right.}{\sum_t^{t\in\etaphi} 1}\quad,
\end{align}
where $t$ runs over the strips in the $\etaphi$ bin.  This correction
is obviously $\IPz$ dependent since the $\etaphi$ bin to which a strip $t$
corresponds to depends on its position relative to the primary
interaction point. 

The 5 output vertex dependent, per--ring histograms of the primary
charged particle density is then given by
\begin{align}
  \dndetadphi[r,\subIPz,i] &=
  \SecMap{} \frac{1}{\DeadCh{}}\dndetadphi[incl,r,\subIPz,i]
\end{align}

\subsection{Histogram collector}
\label{sec:sub:hist_collector}

The histogram collector collects the information from the 5 vertex
dependent, per--ring histograms of the primary charged particle
density $\dndetadphi[r,\subIPz,i]$ into a single vertex dependent
histogram of the charged particle density $\dndetadphi[\subIPz,i]$.

In case the secondary map correction was turned off, as in the case
when the empirical correction is applied, the histogram collector
receives the 5 vertex dependent, per--ring histograms of the
\emph{inclusive} charged particle density
$\dndetadphi[incl,r,\subIPz,i]$ and collect these into a single vertex
dependent histogram of the \emph{inclusive} charged particle density
$\dndetadphi[incl,\subIPz,i]$.  In either case, the same algorithm is
applied, and one can substitute the inclusive histograms for primary
histogram in the below description. 

To do this, we first calculates, for each vertex bin, the $\eta$ bin
range to use for each ring.  We then investigates the secondary
correction maps $\SecMap{}$ to find the edges of each map.  The edges
are given by the $\eta$ range where $\SecMap{}$ is larger than some
threshold\footnote{Typically $t_s\approx 0.1$.}  $t_s$. An additional
safety margin of a number of bins, $N_{cut}$\footnote{Typically
  $N_{cut}=1$.} is applied to ensure that the data selected does not
have too large corrections associated with it.

It then loops over the bins in the defined $\eta$ range and sums the
contributions from each of the 5 histograms.  In the $\eta$ ranges
where two rings overlap, the collector calculates the average and adds
the errors in quadrature\footnote{While not explicitly checked, it was
  found that the histograms agrees within error bars in the
  overlapping region}.

The output, vertex dependent, histogram of the primary
charged particle density is then given by
\begin{align}
  \label{eq:superhist}
  \dndetadphi[\subIPz,i] &=
  \frac{1}{N_{r\in\etaphi}}\sum_{r}^{r\in\etaphi}  
  \dndetadphi[r,\subIPz,i]\\
  \delta\left[\dndetadphi[\subIPz,i]\right] &=
  \frac{1}{N_{r\in\etaphi}}\sqrt{\sum_{r}^{r\in\etaphi}   
    \delta\left[\dndetadphi[r,\subIPz,i]\right]^2}
  \quad,
\end{align}
where $N_{r\in\etaphi}$ is the number of overlapping histograms
in the given $\etaphi$ bin. 

If the secondary map correction was turned of, this becomes 
\begin{align}
  \label{eq:superhist:2}
  \dndetadphi[incl,\subIPz,i] &=
  \frac{1}{N_{r\in\etaphi}}\sum_{r}^{r\in\etaphi}  
  \dndetadphi[incl,r,\subIPz,i]\\
  \delta\left[\dndetadphi[incl,\subIPz,i]\right] &=
  \frac{1}{N_{r\in\etaphi}}\sqrt{\sum_{r}^{r\in\etaphi}   
    \delta\left[\dndetadphi[incl,r,\subIPz,i]\right]^2}
\end{align}

The histogram collector stores the found $\eta$ ranges in the
underflow bin of the histogram produced.  The content of the underflow
bins is %% "Content" is singular
\begin{align}
  \label{eq:underflow}
  \etaCov &= 
  \frac{1}{N_{r\in(\eta)}}
  \sum_{r}^{r\in(\eta)} \left\{\begin{array}{cl} 
      0 & \eta \text{\ bin not selected}\\ 
      1 & \eta \text{\ bin selected}
      \end{array}\right.\quad,
\end{align}
where $N_{r\in(\eta)}$ is the number of overlapping histograms in the
given $\eta$ bin.  The subscript $i$ indicates that the content
depends on the current interaction point of event $i$.

\begin{figure}[]
  \centering
  \figinput[.98\textwidth]{steps}
  \caption{Step--by--step evolution of $\dndeta$ in the analysis.
    Note for this particular analysis, the secondary correction
    $\SecMap{}$ was turned off (see \secref{sec:sub:sub:secmap}). The
    figure should be read from the top-left, downward,
    continuing on the top right, and finally ending up in the lower
    right corner. Dark blue markers are for \FMD{3i}, light blue is
    \FMD{3o}, light green \FMD{2o}, dark green \FMD{2i}, and red
    \FMD{1i}.}
  \label{fig:steps}
\end{figure} 

All the steps involved are summarised in \figref{fig:steps}. 

\subsection{The final \AOD{} output} 

The final \FMD{} \AOD{} is stored on a separate branch\footnote{The
  branch name is \texttt{Forward} for normal data, and
  \texttt{ForwardMC} for simulated data. Both branches store a single
  \texttt{AliAODForwardMult} object per event.} and contains 

\begin{tabular}[T]{p{.2\linewidth}p{.76\linewidth}}
  $\dndetadphi[\subIPz,i]$ & A 2--dimensional histogram of the
                             exclusive number of charged primary
                             particles per $\etaphi$ bin
                             \eqref{eq:superhist}, \emph{or}\\ 
  $\dndetadphi[incl,\subIPz,i]$ & A 2--dimensional histogram of the
                                  inclusive number of charged primary
                                  and secondary particles per
                                  $\etaphi$ bin \eqref{eq:superhist:2}
                                  in case the secondary map correction
                                  was turned off.\\   
  $\phiAcc{}$ & A 1--dimensional histogram of the $\varphi$ acceptance
                per $\eta$ bin \eqref{eq:overflow}. \\
  $\IPz$ & The $z$ coordinate of the interaction point used in the
           above steps.\\
  Trigger bits & A bit field of seen triggers and conditions
                 (pile--up, \SPD{} outlier, etc.) during the above steps.\\ 
\end{tabular}

%% Local Variables:
%%   TeX-master: "PWGLF_Forward_analysis_note.tex"
%%   ispell-dictionary: "british"
%% End:
